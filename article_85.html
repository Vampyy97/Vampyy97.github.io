<html><head><meta charset='utf-8'></head><body><h2>How easily AI governance becomes box-ticking</h2><p class="MsoNormal">The British Standards Institution, the UK body responsible
for developing and publishing standards across various sectors, is right to
raise the alarm over a “wild west” of groups selling audit services that use
artificial intelligence (Report, July 21).</p><p class="MsoNormal">Demand for AI assurance is booming, but so is confusion
about what’s being assessed and who has the credibility to assess it.</p><p class="MsoNormal">Most conversations about AI governance focus on compliance
at the output layer. The reality is, meaningful assurance starts further
upstream, in how models are trained, what data they are trained on, and whether
that data was collected and labelled with fairness and accuracy in mind.</p><p class="MsoNormal">We’ve seen how easily assurance can become a box-ticking
exercise, especially when the same companies building AI are also grading their
own homework. Proprietary audit frameworks may signal progress, but they rarely
offer transparency.</p><p class="MsoNormal">Without visibility into data sourcing, labelling and human
oversight, it’s hard to say what’s being certified, or for whom. Standards like
the BSI’s are a step in the right direction, but they’ll only have teeth if
they reward independence and proximity to the data itself.</p><p class="MsoNormal">Wendy Gonzalez<br/>
CEO, Sama, Los Gatos, CA, US</p></body></html>